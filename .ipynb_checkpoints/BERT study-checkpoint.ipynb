{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Input dataset\n",
    "- NSMC(Naver sentiment movie corpus)/SQUAD/GLUE\n",
    "- 한국 데이터 사이트 (https://sosomemo.tistory.com/21)\n",
    "\n",
    "### NSMC\n",
    "- Naver 영화 리뷰 중 영화 당 100개의 리뷰를 모아 총 200,0000개의 리뷰로 구성\n",
    "- (train data: 15만, test data: 5만)\n",
    "- 긍정(1-4점) / 부정(9-10점) 으로 동일한 비율로 데이터포함\n",
    "<img src='img/nsmc_dataset.png'>\n",
    "\n",
    "* 데이터의 구성은 id, document, label 세개의 열로 구성\n",
    "    * id: 리뷰의 고유한 key 값\n",
    "    * document: 리뷰의 내용\n",
    "    * label: 긍정(0), 부정(1)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from konlpy.utils import pprint\n",
    "import matplotlib as plt\n",
    "from matplotlib import font_manager, rc\n",
    "# font_fname = 'c:/windows/fonts/gulim.ttc'     # A font of your choice\n",
    "# font_name = font_manager.FontProperties(fname=font_fname).get_name()\n",
    "# rc('font', family=font_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shape] train_data: 150000\n",
      "[shape] test_data: 50000\n",
      "/nExample\n",
      "[['9976970', '아 더빙.. 진짜 짜증나네요 목소리', '0'],\n",
      " ['3819312', '흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', '1'],\n",
      " ['10265843', '너무재밓었다그래서보는것을추천한다', '0'],\n",
      " ['9045019', '교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정', '0'],\n",
      " ['6483659',\n",
      "  '사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다',\n",
      "  '1']]\n"
     ]
    }
   ],
   "source": [
    "data_dir = './data/'\n",
    "img_dir = './img/'\n",
    "\n",
    "# Load NSMC data\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        # txt 파일의 헤더(id document label)는 제외하기\n",
    "        data = data[1:]\n",
    "    return data\n",
    "\n",
    "train_data = read_data(data_dir+'ratings_train.txt')\n",
    "test_data = read_data(data_dir+'ratings_test.txt')\n",
    "\n",
    "print('[shape] train_data: '+ str(len(train_data)))\n",
    "print('[shape] test_data: '+ str(len(test_data)))\n",
    "\n",
    "print('/n [Example]')\n",
    "pprint(train_data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soheehwang/anaconda3/lib/python3.6/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['아/Exclamation',\n",
      "  '더빙/Noun',\n",
      "  '../Punctuation',\n",
      "  '진짜/Noun',\n",
      "  '짜증나다/Adjective',\n",
      "  '목소리/Noun'],\n",
      " '0')\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.utils import pprint\n",
    "\n",
    "pos_tagger = Okt()\n",
    "def tokenize(doc):\n",
    "    return['/'.join(t) for t in pos_tagger.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "train_docs = [(tokenize(row[1]), row[2]) for row in train_data]\n",
    "test_docs = [(tokenize(row[1]), row[2]) for row in test_data]\n",
    "\n",
    "pprint(train_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아/Exclamation', '더빙/Noun', '../Punctuation', '진짜/Noun', '짜증나다/Adjective', '목소리/Noun', '흠/Noun', '.../Punctuation', '포스터/Noun', '보고/Noun']\n"
     ]
    }
   ],
   "source": [
    "tokens = [t for d in train_docs for t in d[0]]\n",
    "print(tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = nltk.Text(tokens, name='NMSC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Text: NMSC>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2159921\n",
      "49895\n",
      "[('./Punctuation', 67778),\n",
      " ('영화/Noun', 50818),\n",
      " ('하다/Verb', 41209),\n",
      " ('이/Josa', 38540),\n",
      " ('보다/Verb', 38538),\n",
      " ('의/Josa', 30188),\n",
      " ('../Punctuation', 29055),\n",
      " ('가/Josa', 26627),\n",
      " ('에/Josa', 26468),\n",
      " ('을/Josa', 23118)]\n"
     ]
    }
   ],
   "source": [
    "# Returns number of tokens\n",
    "print(len(text.tokens))\n",
    "\n",
    "# Returns number of unique tokens\n",
    "print(len(set(text.tokens)))\n",
    "\n",
    "# Returns frequency distribution\n",
    "pprint(text.vocab().most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Text' object has no attribute 'collections'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8c482e1fd635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Text' object has no attribute 'collections'"
     ]
    }
   ],
   "source": [
    "# Collocations (연어): 인접하게 빈번하게 등장하는 단어 (ex: \"text\" + \"mining\")\n",
    "\n",
    "text.collocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
